{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying job postings from Indeed.com.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am classifying job postings from Indeed.com.uk.\n",
    "\n",
    "The structure is as follows :\n",
    "\n",
    "\n",
    "1. Create a corpus from a number of job postings.\n",
    "    - This implies scraping the web. For this I used the notebook by https://jessesw.com/Data-Science-Skills/  , which uses the package BeautifulSoup.\n",
    "       \n",
    "2. Create bag-of-word features using Tf-idf. I have used 1,2 and 3-gram bag of words. This is done using TfidfVectorizer from sklearn.feature_extraction.text\n",
    "\n",
    "3. Perform an un-supervised classification of the job-postings with kmeans++ from sklearn.cluster\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function raw_text_cleaner     takes a URL and extracts a job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_text_cleaner(website):\n",
    "    '''\n",
    "    From the notebook by https://jessesw.com/Data-Science-Skills/ \n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        #site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "        site = urllib.request.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    #soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    soup_obj = BeautifulSoup(site, \"lxml\")\n",
    "    if len(soup_obj) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "        soup_obj = BeautifulSoup(site, 'html5lib')\n",
    "    \n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "    text_original = soup_obj.get_text()\n",
    "    text_original = re.sub(\"[^a-zA-Z+3]\",\" \", str(text_original))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','no','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very'])\n",
    "    stop_words = stop_words_base.union(stop_words_jobs)\n",
    "    text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "    text_original = ' '.join(text)\n",
    "\n",
    "    return text_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scientist fospha co skip description searchclose find jobscompany reviewsfind salariesfind cvsemployers post upload cv sign advanced search title keywords city postcode data scientist fospha data scientist variety exciting projects fast growing organization lot tackle complex problems require m\n"
     ]
    }
   ],
   "source": [
    "website = 'https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678'\n",
    "sample_original = raw_text_cleaner('https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678')\n",
    "print(sample_original[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a (1,2,3)-gram bag of words on this sample text, to see what the features look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'ability conduct',\n",
       " 'ability conduct deep',\n",
       " 'able',\n",
       " 'able translate',\n",
       " 'able translate business',\n",
       " 'acquiring',\n",
       " 'acquiring customers',\n",
       " 'acquiring customers attribute',\n",
       " 'acquisition']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,3), sublinear_tf=True)\n",
    "sample_original_features = vectorizer2.fit_transform([sample_original])\n",
    "vectorizer2.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we see how to extract the features from one job posting, let's open several job postings, create a corpus, and create a sparse matrix for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(city = None, job_list=['data+scientist', 'machine+learning'],pages=5):\n",
    "    '''\n",
    "    Initally based on notebook by  https://jessesw.com/Data-Science-Skills/ \n",
    "    Input : city, and a list with job description queries\n",
    "    output: corpus, URLS of jobs descriptions found\n",
    "    '''\n",
    "\n",
    "    if type(job_list == str):\n",
    "        job_list = list(job_list)\n",
    "        \n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    all_URLS=[]\n",
    "    bad_URLS = []\n",
    "    for final_job in job_list:\n",
    "        print('Searching for ', final_job)\n",
    "\n",
    "        #https://www.indeed.co.uk/jobs?q=data+scientist&l=london&sort=date&start=10    \n",
    "        final_site_list = ['https://www.indeed.co.uk/jobs?q=', final_job, '&l=', city,\n",
    "                       '&sort=date'] # Join all of our strings together so that indeed will search correctly        \n",
    "\n",
    "        final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "\n",
    "        base_url = 'https://www.indeed.co.uk'\n",
    "\n",
    "        #print('TRY',final_site)\n",
    "        try:\n",
    "            html = urllib.request.urlopen(final_site).read() # Open up the front page of our search first\n",
    "        except:\n",
    "            'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "            return\n",
    "        soup = BeautifulSoup(html,\"lxml\") # Get the html from the first page\n",
    "        if len(soup) < 1: print('THERE IS AN ERROR LOADING THE PAGE')\n",
    "\n",
    "        # Now find out how many jobs there were\n",
    "\n",
    "        num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                             # The 'searchCount' object has this\n",
    "        print('type(num_jobs_area)')\n",
    "        job_numbers = re.findall('\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result\n",
    "\n",
    "\n",
    "        if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "            total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "        else:\n",
    "            total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "        city_title = city\n",
    "        if city is None:\n",
    "            city_title = 'Nationwide'\n",
    "\n",
    "        print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "\n",
    "        num_pages = int(total_num_jobs/10) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "\n",
    "        #for i in range(1,num_pages+1): # Loop through all of our search result pages\n",
    "        for i in range(0,min(pages,num_pages+1)): # Loop through all of our search result pages\n",
    "            print('Getting page', i)\n",
    "            start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "            if i>0:\n",
    "                current_page = ''.join([final_site, '&start=', start_num])\n",
    "            else:\n",
    "                current_page = final_site\n",
    "            # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            html_page = urllib.request.urlopen(current_page).read() # Get the page\n",
    "\n",
    "            page_obj = BeautifulSoup(html_page,'lxml') # Locate all of the job links\n",
    "            job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "            job_URLS=[]\n",
    "            \n",
    "            for link in job_link_area.find_all('a'):\n",
    "                try:\n",
    "                    if link.get('href')[0:3]=='/rc':\n",
    "                        job_URLS.append(base_url + link.get('href'))\n",
    "                except:\n",
    "                        if link !=None:\n",
    "                            if link.get('href') != None:\n",
    "                                bad_URLS.append(base_url + link.get('href'))\n",
    "\n",
    "            for j in range(0,len(job_URLS)):\n",
    "                final_description = raw_text_cleaner(job_URLS[j])\n",
    "                if final_description: # So that we only append when the website was accessed correctly\n",
    "                    job_descriptions.append(final_description)\n",
    "                    all_URLS.append(job_URLS[j])\n",
    "                #sleep(1) # \n",
    "\n",
    "        print('Done with collecting the job postings!')    \n",
    "        print('There were {} jobs successfully found.'.format(len(job_descriptions)))\n",
    "\n",
    "    return job_descriptions, all_URLS, bad_URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 1: Create the corpus for two very different job descriptions:  'data scientist' and 'restaurant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+scientist\n",
      "type(num_jobs_area)\n",
      "There were 1787 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Done with collecting the job postings!\n",
      "There were 47 jobs successfully found.\n",
      "Searching for  restaurant\n",
      "type(num_jobs_area)\n",
      "There were 14500 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Done with collecting the job postings!\n",
      "There were 72 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus, URLs, bad_URLs = create_corpus(city = 'london',job_list=['data+scientist', 'restaurant'],pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 job postings, 72 URLS\n"
     ]
    }
   ],
   "source": [
    "print('There are {0} job postings, {1} URLS'.format(len(corpus), len(URLs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stop_word_cleaner(corpus, stop_words_input=None):\n",
    "    '''\n",
    "    This function just removes some words from the corpus in case you realize you want to filter out more words.\n",
    "    Inputs: corpus\n",
    "    Outputs: stop_words filtered out of corpus\n",
    "    '''\n",
    "    cc=[0]\n",
    "    if type(corpus) != list: \n",
    "        cc[0] = corpus\n",
    "        corpus = cc\n",
    "        \n",
    "    for ic,text_original in enumerate(corpus):       \n",
    "        stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "        if stop_words_input != None: stop_words_base = stop_words_base.union(stop_words_input)\n",
    "        stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very',\n",
    "                              'cookies','com','asos','postcode','ago','date','benefits',\n",
    "                              'cv','role','cookies','com','asos','postcode','ago','date',\n",
    "                               'benefits','religion','sexual','orientation','salary','asap',\n",
    "                               'annum','race','like' ,'may','enjoy','keywords' ])\n",
    "    \n",
    "        stop_words = stop_words_base.union(stop_words_jobs)\n",
    "        text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "        text_original = ' '.join(text)\n",
    "        corpus[ic] = text_original\n",
    "    if len(corpus) == 1 :\n",
    "        return corpus[ic]\n",
    "    else:\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'junior data scientist farfetch co skip description searchclose find jobscompany reviewsfind salariesfind cvsemployers post upload sign advanced search title keywords city junior data scientist farfetch data science directed building software solutions enhance marketing activity using machine learnin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some additional useless words\n",
    "corpus =  corpus_stop_word_cleaner(corpus, ['cv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'junior data scientist farfetch co skip description searchclose find jobscompany reviewsfind salariesfind cvsemployers post upload sign advanced search title city junior data scientist farfetch data science directed building software solutions enhance marketing activity using machine learning advance'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(corpus, nmin=1,nmax=3,nfeat=5000):    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(nmin,nmax), min_df = 1, \n",
    "                                 sublinear_tf = True, max_features = nfeat)\n",
    "    job_features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, job_features # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectLondon, london_features = create_features(corpus, nmin=1,nmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of extracted features (72, 5000)\n",
      "Some features\n",
      "['able demonstrate', 'able develop', 'able lead', 'abreast', 'academic', 'academic research', 'academy', 'academy aim', 'accept', 'accept right', 'access', 'accessible', 'accessible affordable', 'accomplishments', 'accordance', 'account', 'accounts', 'accounts based', 'accredited', 'accredited qualifications', 'accurately', 'achieve', 'achieve amazing', 'achieves', 'achieving', 'acquisition', 'across', 'across addressable', 'across bbc', 'across broad', 'across business', 'across industrial', 'across markets', 'across sites', 'across whole', 'act', 'acting', 'acting employment', 'action', 'actionable', 'actionable information', 'actions', 'actions organisational', 'active', 'active participant', 'activities', 'activity', 'activity using', 'acumen', 'acumen interested', 'ad', 'ad hoc', 'adam', 'adam description', 'adaptable', 'adaptable able', 'add', 'add business', 'add value', 'addition', 'addition competitive', 'addition plus', 'addition preparing', 'addition regular', 'addition specializes', 'additional', 'additional information', 'address', 'address complex', 'address high', 'addressable', 'addressable channels', 'adhering', 'adhering food', 'administration', 'administration engineering', 'administrative', 'administrative duties', 'administrative personnel', 'administrator']\n",
      "Features are stored in a sparse format <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print('Shape of extracted features',london_features.toarray().shape)\n",
    "print('Some features')\n",
    "print(vectLondon.get_feature_names()[20:100])\n",
    "print('Features are stored in a sparse format',type(london_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 bayesian\n",
      "409 bayesian inference\n",
      "410 bayesian networks\n",
      "411 bayesian statistics\n",
      "2703 optimization bayesian\n",
      "4236 statistics bayesian\n",
      "4419 theory bayesian\n"
     ]
    }
   ],
   "source": [
    "#Let's see how many bayesian-realted features there are \n",
    "for ii,iv in enumerate(vectLondon.get_feature_names()):\n",
    "    if 'bayes' in iv:\n",
    "        print(ii,iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(london_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use these features to do UNSUPERVISED classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_custering(some_features,true_k=2, do_svd=0):\n",
    "    if do_svd:\n",
    "        print(\"Performing dimensionality reduction using LSA\")\n",
    "        # Vectorizer results are normalized, which makes KMeans behave as\n",
    "        # spherical k-means for better results. Since LSA/SVD results are\n",
    "        # not normalized, we have to redo the normalization.\n",
    "        svd = TruncatedSVD(100)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "        X = lsa.fit_transform(some_features)\n",
    "    else:\n",
    "        X = copy.deepcopy(some_features)\n",
    "    \n",
    "    kmeans_minibatch = 0\n",
    "    if kmeans_minibatch:\n",
    "        km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                             init_size=1000, batch_size=1000, verbose=1)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                    verbose=1)\n",
    "\n",
    "    print(\"Clustering sparse data with %s\" % km)\n",
    "    km.fit(X)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=2, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=1)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 126.634\n",
      "Iteration  1, inertia 64.134\n",
      "Iteration  2, inertia 64.054\n",
      "Converged at iteration 2: center shift 0.000000e+00 within tolerance 1.833665e-08\n"
     ]
    }
   ],
   "source": [
    "km = do_custering(london_features,true_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the labels we know we did a reasonable good job, since we know all the first job postings are data science related, and the last ones are restaurant realted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's see which features have more weight  in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_names = vectLondon.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_features(feat_names, km, perc=99.9):\n",
    "    res=km.__dict__\n",
    "    for iclass in set(km.labels_):\n",
    "        print('\\n****** \\nImportant features for class ',iclass,'\\n')\n",
    "        for ii,iv in enumerate(res['cluster_centers_'][iclass]):\n",
    "            if iv > np.percentile(res['cluster_centers_'][iclass],perc)  :\n",
    "                print('{0:<30s}{1}'.format(feat_names[ii],iv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** \n",
      "Important features for class  0 \n",
      "\n",
      "food                          0.043046326334422215\n",
      "front house                   0.03749551893356555\n",
      "great                         0.0459537366124178\n",
      "guests                        0.04170009170266304\n",
      "host                          0.03664484652401619\n",
      "hotel                         0.03882788559878081\n",
      "hotels                        0.04145442090075346\n",
      "kitchen                       0.042782225094246694\n",
      "service                       0.038183750649968005\n",
      "ssp                           0.03655653133185926\n",
      "\n",
      "****** \n",
      "Important features for class  1 \n",
      "\n",
      "data                          0.05498123805199941\n",
      "data science                  0.021964953597315692\n",
      "data scientist                0.04409447506062927\n",
      "help                          0.03384705939705874\n",
      "learning                      0.02806830710367649\n",
      "machine                       0.027179709618865\n",
      "machine learning              0.027131425987046463\n",
      "science                       0.02536939482284588\n",
      "scientist                     0.04542545272965879\n",
      "senior                        0.023013826055767243\n"
     ]
    }
   ],
   "source": [
    "importance_features(feat_names,km, perc=99.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look only at data-science jobs and see what features sets them appart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+scientist\n",
      "type(num_jobs_area)\n",
      "There were 1786 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Getting page 8\n",
      "Getting page 9\n",
      "Done with collecting the job postings!\n",
      "There were 94 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus_ds, URLs_ds, bad_URLs_ds = create_corpus(city = 'london',job_list=['data+scientist'],pages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_ds =  corpus_stop_word_cleaner(corpus_ds,['best','great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectLondon_ds, london_features_ds = create_features(corpus_ds, nmin=1,nmax=3,nfeat=10000)\n",
    "feat_names_ds = vectLondon_ds.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=8, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=1)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 139.622\n",
      "Iteration  1, inertia 73.649\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 9.185831e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 7, 6, 4, 1, 6, 1, 1, 1, 2, 1, 5, 7, 6, 2, 2, 2, 2, 1, 1, 3, 1,\n",
       "       1, 7, 0, 1, 6, 2, 1, 2, 2, 0, 2, 0, 7, 1, 2, 2, 0, 2, 6, 7, 2, 2, 2,\n",
       "       2, 7, 1, 2, 2, 2, 1, 1, 0, 7, 2, 1, 4, 3, 3, 2, 1, 4, 2, 6, 7, 0, 1,\n",
       "       5, 7, 5, 2, 2, 3, 1, 1, 7, 7, 3, 4, 2, 4, 1, 0, 1, 6, 1, 6, 6, 6, 4,\n",
       "       3, 2], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's arbitrarily split them into 4 clusters\n",
    "km_ds = do_custering(london_features_ds,true_k=8)\n",
    "km_ds.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "94\n",
      "(94, 10000)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_ds))\n",
    "print(len(km_ds.labels_))\n",
    "print(london_features_ds.shape)\n",
    "print(len(feat_names_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** \n",
      "Important features for class  0 \n",
      "\n",
      "analytics                     0.04604695159388804\n",
      "aspire                        0.07397985857762232\n",
      "aspire data                   0.07905726454155611\n",
      "aspire data recruitment       0.07905726454155611\n",
      "bi analyst                    0.03696233272123313\n",
      "data                          0.06262106025756638\n",
      "data recruitment              0.07905726454155611\n",
      "data scientist                0.05232207509922674\n",
      "data scientist aspire         0.03796794367499669\n",
      "etc                           0.03700205106992695\n",
      "marketing                     0.03607180393174988\n",
      "marketing analytics           0.04286130883282559\n",
      "modelling                     0.05824819132252552\n",
      "python                        0.03663127686155278\n",
      "recruitment                   0.04678154948975609\n",
      "scientist                     0.046862274529366164\n",
      "statistician                  0.05917793853204608\n",
      "\n",
      "****** \n",
      "Important features for class  1 \n",
      "\n",
      "big                           0.02701058305356886\n",
      "business                      0.02403503225173146\n",
      "data                          0.033931451461051955\n",
      "data scientists               0.021476828998903692\n",
      "design                        0.02202209599898583\n",
      "developer                     0.02480447879429152\n",
      "engineer                      0.029584649915575113\n",
      "engineers                     0.022098751371812334\n",
      "help                          0.027402308883110512\n",
      "java                          0.02056653201706212\n",
      "limited                       0.023100997698792383\n",
      "people                        0.027614609959499024\n",
      "platform                      0.02525308815571497\n",
      "product                       0.024550575472049706\n",
      "scientists                    0.022692030978947498\n",
      "services                      0.02217814095873771\n",
      "software                      0.028159627104041886\n",
      "solutions                     0.02220227441790656\n",
      "technology                    0.0286457715592464\n",
      "zopa                          0.02456664765139712\n",
      "\n",
      "****** \n",
      "Important features for class  2 \n",
      "\n",
      "applause                      0.027026541716551538\n",
      "data                          0.05760625169504209\n",
      "data scientist                0.06858782684793986\n",
      "data scientist applause       0.027026541716551538\n",
      "data scientist premier        0.04092905937830616\n",
      "help                          0.03102530808531875\n",
      "junior                        0.028126625095097908\n",
      "junior data                   0.02768931936082119\n",
      "lead                          0.037453561233947795\n",
      "lead data                     0.041406755390544385\n",
      "lead data scientist           0.04216784389732427\n",
      "ltd                           0.03238681019626059\n",
      "machine                       0.027021775331792574\n",
      "machine learning              0.027021775331792574\n",
      "premier                       0.05114063753840802\n",
      "recruitment                   0.027455118501286575\n",
      "science                       0.03041222078783515\n",
      "scientist                     0.06414039874692978\n",
      "scientist applause            0.027026541716551538\n",
      "scientist premier             0.04092905937830616\n",
      "\n",
      "****** \n",
      "Important features for class  3 \n",
      "\n",
      "account                       0.03461723725810404\n",
      "account manager               0.03316508344517253\n",
      "campaign                      0.043506393962578827\n",
      "campaign manager              0.03525272308386167\n",
      "campaigns                     0.04276997264453028\n",
      "digital                       0.04041954805288211\n",
      "every                         0.031604046618124756\n",
      "fantastic                     0.032353544862091556\n",
      "growth                        0.03343003987806775\n",
      "manager                       0.054135538006749355\n",
      "marketing                     0.051712757673337856\n",
      "media                         0.03167932268613884\n",
      "merkle                        0.04042105651300699\n",
      "merkle periscopix             0.0380237993657055\n",
      "month                         0.0347413342182502\n",
      "must                          0.03528907780090194\n",
      "periscopix                    0.04042105651300699\n",
      "ppc                           0.04116252352195333\n",
      "rapidminer                    0.04646740394502225\n",
      "sales                         0.046189181191679286\n",
      "\n",
      "****** \n",
      "Important features for class  4 \n",
      "\n",
      "agencies                      0.039577871806314065\n",
      "authorization                 0.03805251363954345\n",
      "biological                    0.032875836453185286\n",
      "businesses agencies           0.03805251363954345\n",
      "consumer                      0.045487199232606156\n",
      "contact                       0.034164842834428305\n",
      "department                    0.034647084751984344\n",
      "employment                    0.04094881556542341\n",
      "employment businesses         0.04326883017639699\n",
      "employment businesses agencies0.03805251363954345\n",
      "gsk                           0.06497079990779461\n",
      "health                        0.04681383829641539\n",
      "healthcare                    0.037876926232670284\n",
      "human                         0.034321551774987795\n",
      "pharmaceutical                0.03384420363963702\n",
      "safety                        0.034627764654300015\n",
      "senior scientist              0.039466529946126615\n",
      "skin                          0.03824794199801027\n",
      "skin health                   0.032866721813862555\n",
      "written authorization         0.03805251363954345\n",
      "\n",
      "****** \n",
      "Important features for class  5 \n",
      "\n",
      "aspire data                   0.0539345351390264\n",
      "aspire data recruitment       0.0539345351390264\n",
      "data recruitment              0.0539345351390264\n",
      "decision                      0.07354848883060927\n",
      "decision science              0.05812118566553894\n",
      "decision scientist            0.09508635874963385\n",
      "deliver actionable            0.06715725914806912\n",
      "fortran                       0.05197462681674636\n",
      "insight                       0.06178924595871737\n",
      "intuitive                     0.052487213888779506\n",
      "java python fortran           0.05197462681674636\n",
      "mathematical                  0.06108334130456356\n",
      "mathematics                   0.0550562747926381\n",
      "mathematics statistics        0.05600737658298499\n",
      "phd decision                  0.06469443342502579\n",
      "phd decision scientist        0.06469443342502579\n",
      "problem solving               0.05175705159524832\n",
      "python fortran                0.05197462681674636\n",
      "quantitative                  0.05406617402152771\n",
      "trends                        0.05408976731482188\n",
      "\n",
      "****** \n",
      "Important features for class  6 \n",
      "\n",
      "analyst                       0.04959403773956601\n",
      "chase                         0.048747670987778796\n",
      "citi                          0.034166698601074665\n",
      "corporate                     0.04215878356698081\n",
      "investment                    0.040852400827521146\n",
      "jp                            0.03880074350900267\n",
      "jp morgan                     0.03880074350900267\n",
      "jp morgan chase               0.03880074350900267\n",
      "jpmorgan                      0.04127834365084868\n",
      "jpmorgan chase                0.03970725341942069\n",
      "model                         0.04569010939744633\n",
      "model risk                    0.03661127329257292\n",
      "morgan                        0.043457555597606765\n",
      "morgan chase                  0.03928648500053933\n",
      "quant                         0.03570717527579782\n",
      "quantitative                  0.04050202089118586\n",
      "quantitative analyst          0.04600651398971032\n",
      "reviews                       0.03348781987490582\n",
      "risk                          0.044427469930234484\n",
      "trading                       0.03773950216412662\n",
      "\n",
      "****** \n",
      "Important features for class  7 \n",
      "\n",
      "ai                            0.045594950661503736\n",
      "ai machine                    0.028736945527949576\n",
      "ai machine learning           0.028736945527949576\n",
      "algorithms                    0.02744331209227413\n",
      "babylon                       0.029595800169542576\n",
      "complyadvantage               0.027634360275609064\n",
      "days                          0.027874912787125963\n",
      "developer                     0.03082616784085278\n",
      "engineer                      0.03686879470929639\n",
      "help                          0.026904557410849475\n",
      "learning                      0.05700295068417293\n",
      "learning engineer             0.04878935117869771\n",
      "looking                       0.026712148479348177\n",
      "machine                       0.05930310493525693\n",
      "machine learning              0.05908078940080625\n",
      "machine learning engineer     0.04878935117869771\n",
      "ml                            0.030673799827035133\n",
      "recruitment                   0.02781898379471787\n",
      "ruby                          0.026520314227242592\n",
      "senior                        0.035565953765870385\n"
     ]
    }
   ],
   "source": [
    "importance_features(feat_names_ds, km_ds, perc=99.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at each category, the job adds can be described as:\n",
    "    0. not clear\n",
    "    1. developer, engineer, \n",
    "    2. data scientist\n",
    "    3. marketing, meaida, campaign\n",
    "    4. health, employment\n",
    "    5. phd, mathematics\n",
    "    6. financial\n",
    "    7. AI- ml\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, you're job search is much easier!\n",
    "\n",
    "## Imagine you're only interested in data science roles that have a focus in finance. In that case, you can target those jobs specially:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def provide_url_one_class(km,URL, target_category):\n",
    "    target_url=[]\n",
    "    for i,ilab in enumerate(km.labels_):\n",
    "        if ilab in target_category:\n",
    "            print(URL[i])\n",
    "            target_url.append(URL[i])\n",
    "    return target_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.co.uk/rc/clk?jk=d9a8e791fdb670fa&fccid=1196f2e3f43d848d&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=7840de748811589f&fccid=36801496409e6dc9&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=33856e24365bae94&fccid=c46d0116f6e69eae&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=19808101cc3f5559&fccid=5bcd1ef0a7f4fb99&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=23de10bb768ae4fb&fccid=5bcd1ef0a7f4fb99&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=b99426fe9605e04b&fccid=3c0bf511b4a29309&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=02cc5d235730ef8d&fccid=c46d0116f6e69eae&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=603427c2c8c8a75d&fccid=df6948c9b8da6236&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=a7011e68e4bbc26a&fccid=c46d0116f6e69eae&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=69be5747ea363ebb&fccid=c46d0116f6e69eae&vjs=3\n"
     ]
    }
   ],
   "source": [
    "my_url = provide_url_one_class(km_ds,URLs_ds, [6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in a more mathematical position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.co.uk/rc/clk?jk=0190224aaa202c4b&fccid=cbf7c87b1ccf4a6c&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=52cb1c590d212275&fccid=0b33f99aac420958&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=64a76f928b12dedc&fccid=0b33f99aac420958&vjs=3\n"
     ]
    }
   ],
   "source": [
    "my_url = provide_url_one_class(km_ds,URLs_ds, [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
