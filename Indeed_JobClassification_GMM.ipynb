{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying job postings from Indeed.com.uk with Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am classifying job postings from Indeed.com.uk.\n",
    "\n",
    "The structure is as follows :\n",
    "\n",
    "\n",
    "1. Create a corpus from a number of job postings.\n",
    "    - This implies scraping the web. For this I used the notebook by https://jessesw.com/Data-Science-Skills/  , which uses the package BeautifulSoup.\n",
    "       \n",
    "2. Create bag-of-word features using Tf-idf. I have used 1,2 and 3-gram bag of words. This is done using TfidfVectorizer from sklearn.feature_extraction.text\n",
    "\n",
    "3. Perform an un-supervised classification of the job-postings with kmeans++ from sklearn.cluster\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function raw_text_cleaner     takes a URL and extracts a job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_text_cleaner(website):\n",
    "    '''\n",
    "    From the notebook by https://jessesw.com/Data-Science-Skills/ \n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        #site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "        site = urllib.request.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    #soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    soup_obj = BeautifulSoup(site, \"lxml\")\n",
    "    if len(soup_obj) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "        soup_obj = BeautifulSoup(site, 'html5lib')\n",
    "    \n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "    text_original = soup_obj.get_text()\n",
    "    text_original = re.sub(\"[^a-zA-Z+3]\",\" \", str(text_original))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','no','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very'])\n",
    "    stop_words = stop_words_base.union(stop_words_jobs)\n",
    "    text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "    text_original = ' '.join(text)\n",
    "\n",
    "    return text_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scientist fospha co skip description searchclose find jobscompany reviewsfind salariesfind cvsemployers post upload cv sign advanced search title keywords city postcode data scientist fospha data scientist variety exciting projects fast growing organization lot tackle complex problems require m\n"
     ]
    }
   ],
   "source": [
    "website = 'https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678'\n",
    "sample_original = raw_text_cleaner('https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678')\n",
    "print(sample_original[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a (1,2,3)-gram bag of words on this sample text, to see what the features look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'ability conduct',\n",
       " 'ability conduct deep',\n",
       " 'able',\n",
       " 'able translate',\n",
       " 'able translate business',\n",
       " 'acquiring',\n",
       " 'acquiring customers',\n",
       " 'acquiring customers attribute',\n",
       " 'acquisition']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,3), sublinear_tf=True)\n",
    "sample_original_features = vectorizer2.fit_transform([sample_original])\n",
    "vectorizer2.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we see how to extract the features from one job posting, let's open several job postings, create a corpus, and create a sparse matrix for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(city = None, job_list=['data+scientist', 'machine+learning'],pages=5):\n",
    "    '''\n",
    "    Initally based on notebook by  https://jessesw.com/Data-Science-Skills/ \n",
    "    Input : city, and a list with job description queries\n",
    "    output: corpus, URLS of jobs descriptions found\n",
    "    '''\n",
    "\n",
    "    if type(job_list == str):\n",
    "        job_list = list(job_list)\n",
    "        \n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    all_URLS=[]\n",
    "    bad_URLS = []\n",
    "    for final_job in job_list:\n",
    "        print('Searching for ', final_job)\n",
    "\n",
    "        #https://www.indeed.co.uk/jobs?q=data+scientist&l=london&sort=date&start=10    \n",
    "        final_site_list = ['https://www.indeed.co.uk/jobs?q=', final_job, '&l=', city,\n",
    "                       '&sort=date'] # Join all of our strings together so that indeed will search correctly        \n",
    "\n",
    "        final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "\n",
    "        base_url = 'https://www.indeed.co.uk'\n",
    "\n",
    "        #print('TRY',final_site)\n",
    "        try:\n",
    "            html = urllib.request.urlopen(final_site).read() # Open up the front page of our search first\n",
    "        except:\n",
    "            'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "            return\n",
    "        soup = BeautifulSoup(html,\"lxml\") # Get the html from the first page\n",
    "        if len(soup) < 1: print('THERE IS AN ERROR LOADING THE PAGE')\n",
    "\n",
    "        # Now find out how many jobs there were\n",
    "\n",
    "        num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                             # The 'searchCount' object has this\n",
    "        print('type(num_jobs_area)')\n",
    "        job_numbers = re.findall('\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result\n",
    "\n",
    "\n",
    "        if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "            total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "        else:\n",
    "            total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "        city_title = city\n",
    "        if city is None:\n",
    "            city_title = 'Nationwide'\n",
    "\n",
    "        print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "\n",
    "        num_pages = int(total_num_jobs/10) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "\n",
    "        for i in range(0,min(pages,num_pages+1)): # Loop through all of our search result pages\n",
    "            print('Getting page', i)\n",
    "            start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "            if i>0:\n",
    "                current_page = ''.join([final_site, '&start=', start_num])\n",
    "            else:\n",
    "                current_page = final_site\n",
    "            # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            html_page = urllib.request.urlopen(current_page).read() # Get the page\n",
    "\n",
    "            page_obj = BeautifulSoup(html_page,'lxml') # Locate all of the job links\n",
    "            job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "            job_URLS=[]\n",
    "            \n",
    "            for link in job_link_area.find_all('a'):\n",
    "                try:\n",
    "                    if link.get('href')[0:3]=='/rc':\n",
    "                        job_URLS.append(base_url + link.get('href'))\n",
    "                except:\n",
    "                        if link !=None:\n",
    "                            if link.get('href') != None:\n",
    "                                bad_URLS.append(base_url + link.get('href'))\n",
    "\n",
    "            for j in range(0,len(job_URLS)):\n",
    "                final_description = raw_text_cleaner(job_URLS[j])\n",
    "                if final_description: # So that we only append when the website was accessed correctly\n",
    "                    job_descriptions.append(final_description)\n",
    "                    all_URLS.append(job_URLS[j])\n",
    "                #sleep(1) # \n",
    "\n",
    "        print('Done with collecting the job postings!')    \n",
    "        print('There were {} jobs successfully found.'.format(len(job_descriptions)))\n",
    "\n",
    "    return job_descriptions, all_URLS, bad_URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 1: Create the corpus for two very different job descriptions:  'data scientist' and 'restaurant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+scientist\n",
      "type(num_jobs_area)\n",
      "There were 1778 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Done with collecting the job postings!\n",
      "There were 27 jobs successfully found.\n",
      "Searching for  waiter\n",
      "type(num_jobs_area)\n",
      "There were 1454 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Done with collecting the job postings!\n",
      "There were 43 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus, URLs, bad_URLs = create_corpus(city = 'london',job_list=['data+scientist', 'waiter'],pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43 job postings, 43 URLS\n"
     ]
    }
   ],
   "source": [
    "print('There are {0} job postings, {1} URLS'.format(len(corpus), len(URLs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_stop_word_cleaner(corpus, stop_words_input=None):\n",
    "    '''\n",
    "    This function just removes some words from the corpus in case you realize you want to filter out more words.\n",
    "    Inputs: corpus\n",
    "    Outputs: stop_words filtered out of corpus\n",
    "    '''\n",
    "    cc=[0]\n",
    "    if type(corpus) != list: \n",
    "        cc[0] = corpus\n",
    "        corpus = cc\n",
    "        \n",
    "    for ic,text_original in enumerate(corpus):       \n",
    "        stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "        if stop_words_input != None: stop_words_base = stop_words_base.union(stop_words_input)\n",
    "        stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very',\n",
    "                              'cookies','com','asos','postcode','ago','date','benefits',\n",
    "                              'cv','role','cookies','com','asos','postcode','ago','date',\n",
    "                               'benefits','religion','sexual','orientation','salary','asap',\n",
    "                               'annum','race','like' ,'may','enjoy','keywords' ])\n",
    "    \n",
    "        stop_words = stop_words_base.union(stop_words_jobs)\n",
    "        text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "        text_original = ' '.join(text)\n",
    "        corpus[ic] = text_original\n",
    "    if len(corpus) == 1 :\n",
    "        return corpus[ic]\n",
    "    else:\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove some additional useless words\n",
    "corpus =  corpus_stop_word_cleaner(corpus, ['cv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(corpus, nmin=1,nmax=3,nfeat=10000):    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(nmin,nmax), min_df = 1, \n",
    "                                 sublinear_tf = True, max_features = nfeat)\n",
    "    job_features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, job_features # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectLondon, london_features = create_features(corpus, nmin=1,nmax=3, nfeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of extracted features (43, 100)\n",
      "Some features\n",
      "['deliver', 'deliver services', 'deliver services cookie', 'describes', 'describes use', 'describes use disable', 'description', 'description searchclose', 'description searchclose find', 'development', 'disable', 'disable learn', 'disable learn ok', 'engineer', 'find', 'find jobscompany', 'find jobscompany reviewsfind', 'help', 'help centre', 'help centre help', 'help deliver', 'help deliver services', 'jobscompany', 'jobscompany reviewsfind', 'jobscompany reviewsfind salariesfind', 'learn', 'learn ok', 'learn ok anti', 'learning', 'ok', 'ok anti', 'ok anti statement', 'one', 'original', 'plan', 'policy', 'policy describes', 'policy describes use', 'post', 'post upload', 'post upload sign', 'privacy', 'privacy terms', 'reviewsfind', 'reviewsfind salariesfind', 'reviewsfind salariesfind cvsemployers', 'salariesfind', 'salariesfind cvsemployers', 'salariesfind cvsemployers post', 'save', 'save original', 'scientist', 'search', 'search title', 'search title city', 'searchclose', 'searchclose find', 'searchclose find jobscompany', 'services', 'services cookie', 'services cookie policy', 'sign', 'sign advanced', 'sign advanced search', 'skip', 'skip description', 'skip description searchclose', 'statement', 'statement privacy', 'statement privacy terms', 'terms', 'title', 'title city', 'upload', 'upload sign', 'upload sign advanced', 'use', 'use disable', 'use disable learn', 'waiter']\n",
      "Features are stored in a sparse format <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print('Shape of extracted features',london_features.toarray().shape)\n",
    "print('Some features')\n",
    "print(vectLondon.get_feature_names()[20:100])\n",
    "print('Features are stored in a sparse format',type(london_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 100)\n"
     ]
    }
   ],
   "source": [
    "print(london_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning engineer aig co skip description searchclose find jobscompany reviewsfind salariesfind\n"
     ]
    }
   ],
   "source": [
    "print(corpus[3][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use these features to do UNSUPERVISED classification with GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "cov_type='diag'\n",
    "estimator1 = GaussianMixture(n_components=k,\n",
    "                   covariance_type=cov_type, max_iter=100, random_state=0)\n",
    "estimator1.fit(london_features.toarray()  )    # Learns model parameters\n",
    "y_pred = estimator1.predict(london_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the labels we know we did a reasonable good job, since we know all the first job postings are data science related, and the last ones are realted to waiters jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's see which features have more weight  in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_names = vectLondon.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_features(k, feature_names,estimator,perc=99.9):\n",
    "    for ik in range(k):        \n",
    "        impor_feat = {}\n",
    "        muik = estimator.means_[ik]\n",
    "        tth = np.percentile(muik,perc)\n",
    "        for i,iv in enumerate(muik):\n",
    "            if iv > tth:\n",
    "                impor_feat[feature_names[i] ] =  iv \n",
    "        sorted_x = sorted(impor_feat.items(), key=operator.itemgetter(1),reverse=True  )\n",
    "        print('\\n \\n Important features for cluster ', ik)\n",
    "        for ii in sorted_x:\n",
    "             print('{0:<30s}{1}'.format(ii[0],ii[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Important features for cluster  0\n",
      "data                          0.30724879366775093\n",
      "engineer                      0.17864637694965696\n",
      "scientist                     0.15368974823220527\n",
      "help                          0.15113825447693663\n",
      "learning                      0.15076261773119337\n",
      "\n",
      " \n",
      " Important features for cluster  1\n",
      "waiter                        0.35717768617518614\n",
      "help                          0.1613751747678486\n",
      "learn                         0.10780829146340816\n",
      "deliver                       0.10539187608063731\n",
      "find                          0.0988078090306612\n"
     ]
    }
   ],
   "source": [
    "importance_features(k,feat_names,estimator1,perc=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look only at data-science jobs and see what features sets them appart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+science\n",
      "type(num_jobs_area)\n",
      "There were 5293 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Done with collecting the job postings!\n",
      "There were 56 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus_ds, URLs_ds, bad_URLs_ds = create_corpus(city = 'london',job_list=['data+science'],pages=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_ds =  corpus_stop_word_cleaner(corpus_ds,['ck','best','great','boyce','mendez','mondelez','durlston','cwjobs','futureheads','harnham','venturi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectLondon_ds, london_features_ds = create_features(corpus_ds, nmin=1,nmax=3,nfeat=1000)\n",
    "feat_names_ds = vectLondon_ds.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "cov_type='diag'\n",
    "estimator2 = GaussianMixture(n_components=k,\n",
    "                   covariance_type=cov_type, max_iter=300, random_state=0)\n",
    "estimator2.fit(london_features_ds.toarray()  )    # Learns model parameters\n",
    "y_pred = estimator2.predict(london_features_ds.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 4, 2, 0, 1, 0, 2, 5, 3, 5, 7, 2, 7, 7, 3, 7, 2, 0, 0, 0, 5, 1,\n",
       "       6, 2, 2, 2, 1, 6, 6, 2, 3, 7, 1, 7, 7, 2, 2, 2, 6, 1, 1, 2, 6, 0, 2,\n",
       "       2, 2, 4, 2, 1, 6, 2, 1, 5, 6])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Important features for cluster  0\n",
      "engineer                      0.13678746949644155\n",
      "software                      0.08559024383915062\n",
      "aws                           0.08116265602017429\n",
      "software engineer             0.07972412404219022\n",
      "product                       0.0763694882249686\n",
      "media                         0.0736189383020986\n",
      "digital                       0.06650538986881856\n",
      "business                      0.06617113641471413\n",
      "development                   0.06561114006135321\n",
      "technical                     0.05814401334860535\n",
      "\n",
      " \n",
      " Important features for cluster  1\n",
      "scientist                     0.111998240250204\n",
      "data scientist                0.10809198320696624\n",
      "data science                  0.0730476656066979\n",
      "data                          0.07243356979037488\n",
      "ey                            0.0698053020591506\n",
      "analytics                     0.06472596889054012\n",
      "public health                 0.062348210126380005\n",
      "health                        0.060985164976196944\n",
      "33                            0.060655004231571046\n",
      "33 reviews                    0.060655004231571046\n",
      "\n",
      " \n",
      " Important features for cluster  2\n",
      "data                          0.05673562600590759\n",
      "business                      0.04636236646195351\n",
      "analyst                       0.046071963581409345\n",
      "knowledge                     0.04452211440315544\n",
      "help                          0.04401846812889093\n",
      "customer                      0.04370028098062057\n",
      "using                         0.03984677073788914\n",
      "analytics                     0.03871221725859589\n",
      "strategy                      0.03818018889822686\n",
      "services                      0.037137460707783274\n",
      "\n",
      " \n",
      " Important features for cluster  3\n",
      "security                      0.14535149127521962\n",
      "cyber                         0.13878886105735083\n",
      "cyber security                0.11988730874055573\n",
      "point                         0.10189533359646093\n",
      "infrastructure                0.09333846871004449\n",
      "engineers                     0.09294375765288662\n",
      "certification                 0.09191763875389426\n",
      "cyber security engineer       0.08736136087318348\n",
      "security engineer             0.08736136087318348\n",
      "cloud                         0.08681076478938146\n",
      "\n",
      " \n",
      " Important features for cluster  4\n",
      "research                      0.1644219192768757\n",
      "academic                      0.162001130108066\n",
      "city university               0.15640110588009587\n",
      "survey                        0.14182257954214117\n",
      "european                      0.12562406766382178\n",
      "university                    0.12484054391091683\n",
      "resourcing                    0.12085457101084522\n",
      "school                        0.11488144740239808\n",
      "global                        0.1037059721719441\n",
      "social                        0.09438622595862711\n",
      "\n",
      " \n",
      " Important features for cluster  5\n",
      "developer                     0.1949019528194032\n",
      "full stack developer          0.12372072251818561\n",
      "stack developer               0.12372072251818561\n",
      "client server                 0.12145060734367935\n",
      "full stack                    0.11469732678377248\n",
      "python developer              0.11128458665689281\n",
      "stack                         0.10250135077059712\n",
      "programmer                    0.08892137746064586\n",
      "java                          0.08527516658711452\n",
      "server                        0.08496277588598303\n",
      "\n",
      " \n",
      " Important features for cluster  6\n",
      "business                      0.07309656996135659\n",
      "reporting                     0.07239186762357779\n",
      "user                          0.0656708943348268\n",
      "sql developer                 0.06112046081660881\n",
      "developer                     0.05900875355494469\n",
      "requirements                  0.05736723105227698\n",
      "sql                           0.05444747098369314\n",
      "analyst                       0.05409923729705313\n",
      "credit                        0.05279741879450818\n",
      "risk                          0.05247952204352583\n",
      "\n",
      " \n",
      " Important features for cluster  7\n",
      "content                       0.10108741703211226\n",
      "project                       0.0951238267225925\n",
      "clients                       0.07998786360587302\n",
      "creative                      0.07771891774696166\n",
      "client                        0.07575000261462882\n",
      "marketing                     0.06904923723351546\n",
      "digital                       0.0684569467451684\n",
      "manager                       0.06786359185084936\n",
      "project management            0.06638786835595954\n",
      "media                         0.06342010985182497\n"
     ]
    }
   ],
   "source": [
    "importance_features(k, feat_names_ds, estimator2, perc=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at each category, and find the best one(s) suited for you. \n",
    "\n",
    "### Now, you're job search is much easier!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def provide_url_one_class(label,URL, target_category):\n",
    "    target_url=[]\n",
    "    for i,ilab in enumerate(label):\n",
    "        if ilab in target_category:\n",
    "            print(URL[i])\n",
    "            target_url.append(URL[i])\n",
    "    return target_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.co.uk/rc/clk?jk=1cb0ace166ce0185&fccid=892c145157842ceb&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=e6474828284e2918&fccid=892c145157842ceb&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=0b1e74d40435441e&fccid=0592bb9a425e26cc&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=657e9439763ef114&fccid=113037d2ecac6197&vjs=3\n"
     ]
    }
   ],
   "source": [
    "#lets look at developer-oriented data science jobs\n",
    "my_url = provide_url_one_class(y_pred,URLs_ds, [5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at estimator2.weights_   we can see that in this case the largest contributions are from category 2 and category 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10714286,  0.14285714,  0.30357143,  0.07142857,  0.03571429,\n",
       "        0.07142857,  0.14285714,  0.125     ])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator2.weights_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
