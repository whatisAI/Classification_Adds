{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying job postings from Indeed.com.uk with Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am classifying job postings from Indeed.com.uk.\n",
    "\n",
    "The structure is as follows :\n",
    "\n",
    "\n",
    "1. Create a corpus from a number of job postings.\n",
    "    - This implies scraping the web. For this I used the notebook by https://jessesw.com/Data-Science-Skills/  , which uses the package BeautifulSoup.\n",
    "       \n",
    "2. Create bag-of-word features using Tf-idf. I have used 1,2 and 3-gram bag of words. This is done using TfidfVectorizer from sklearn.feature_extraction.text\n",
    "\n",
    "3. Perform an un-supervised classification of the job-postings with kmeans++ from sklearn.cluster\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function raw_text_cleaner     takes a URL and extracts a job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_text_cleaner(website):\n",
    "    '''\n",
    "    From the notebook by https://jessesw.com/Data-Science-Skills/ \n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        #site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "        site = urllib.request.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    #soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    soup_obj = BeautifulSoup(site, \"lxml\")\n",
    "    if len(soup_obj) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "        soup_obj = BeautifulSoup(site, 'html5lib')\n",
    "    \n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "    text_original = soup_obj.get_text()\n",
    "    text_original = re.sub(\"[^a-zA-Z+3]\",\" \", str(text_original))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','no','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very'])\n",
    "    stop_words = stop_words_base.union(stop_words_jobs)\n",
    "    text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "    text_original = ' '.join(text)\n",
    "\n",
    "    return text_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scientist fospha co skip description searchclose find jobscompany reviewsfind salariesfind cvsemployers post upload cv sign advanced search title keywords city postcode data scientist fospha data scientist variety exciting projects fast growing organization lot tackle complex problems require m\n"
     ]
    }
   ],
   "source": [
    "website = 'https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678'\n",
    "sample_original = raw_text_cleaner('https://www.indeed.co.uk/viewjob?jk=ba3df8f30e1691b7&tk=1c82t0tcm9m5i9ng&from=serp&alid=3&advn=1402909195792678')\n",
    "print(sample_original[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a (1,2,3)-gram bag of words on this sample text, to see what the features look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'ability conduct',\n",
       " 'ability conduct deep',\n",
       " 'able',\n",
       " 'able translate']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,3), sublinear_tf=True)\n",
    "sample_original_features = vectorizer2.fit_transform([sample_original])\n",
    "vectorizer2.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we see how to extract the features from one job posting, let's open several job postings, create a corpus, and create a sparse matrix for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(city = None, job_list=['data+scientist', 'machine+learning'],pages=5):\n",
    "    '''\n",
    "    Initally based on notebook by  https://jessesw.com/Data-Science-Skills/ \n",
    "    Input : city, and a list with job description queries\n",
    "    output: corpus, URLS of jobs descriptions found\n",
    "    '''\n",
    "\n",
    "    if type(job_list == str):\n",
    "        job_list = list(job_list)\n",
    "        \n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    all_URLS=[]\n",
    "    bad_URLS = []\n",
    "    for final_job in job_list:\n",
    "        print('Searching for ', final_job)\n",
    "\n",
    "        #https://www.indeed.co.uk/jobs?q=data+scientist&l=london&sort=date&start=10    \n",
    "        final_site_list = ['https://www.indeed.co.uk/jobs?q=', final_job, '&l=', city,\n",
    "                       '&sort=date'] # Join all of our strings together so that indeed will search correctly        \n",
    "\n",
    "        final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "\n",
    "        base_url = 'https://www.indeed.co.uk'\n",
    "\n",
    "        #print('TRY',final_site)\n",
    "        try:\n",
    "            html = urllib.request.urlopen(final_site).read() # Open up the front page of our search first\n",
    "        except:\n",
    "            'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "            return\n",
    "        soup = BeautifulSoup(html,\"lxml\") # Get the html from the first page\n",
    "        if len(soup) < 1: print('THERE IS AN ERROR LOADING THE PAGE')\n",
    "\n",
    "        # Now find out how many jobs there were\n",
    "\n",
    "        num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                             # The 'searchCount' object has this\n",
    "        print('type(num_jobs_area)')\n",
    "        job_numbers = re.findall('\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result\n",
    "\n",
    "\n",
    "        if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "            total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "        else:\n",
    "            total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "        city_title = city\n",
    "        if city is None:\n",
    "            city_title = 'Nationwide'\n",
    "\n",
    "        print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "\n",
    "        num_pages = int(total_num_jobs/10) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "\n",
    "        for i in range(0,min(pages,num_pages+1)): # Loop through all of our search result pages\n",
    "            print('Getting page', i)\n",
    "            start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "            if i>0:\n",
    "                current_page = ''.join([final_site, '&start=', start_num])\n",
    "            else:\n",
    "                current_page = final_site\n",
    "            # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            html_page = urllib.request.urlopen(current_page).read() # Get the page\n",
    "\n",
    "            page_obj = BeautifulSoup(html_page,'lxml') # Locate all of the job links\n",
    "            job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "            job_URLS=[]\n",
    "            \n",
    "            for link in job_link_area.find_all('a'):\n",
    "                try:\n",
    "                    if link.get('href')[0:3]=='/rc':\n",
    "                        job_URLS.append(base_url + link.get('href'))\n",
    "                except:\n",
    "                        if link !=None:\n",
    "                            if link.get('href') != None:\n",
    "                                bad_URLS.append(base_url + link.get('href'))\n",
    "\n",
    "            for j in range(0,len(job_URLS)):\n",
    "                final_description = raw_text_cleaner(job_URLS[j])\n",
    "                if final_description: # So that we only append when the website was accessed correctly\n",
    "                    job_descriptions.append(final_description)\n",
    "                    all_URLS.append(job_URLS[j])\n",
    "                #sleep(1) # \n",
    "\n",
    "        print('Done with collecting the job postings!')    \n",
    "        print('There were {} jobs successfully found.'.format(len(job_descriptions)))\n",
    "\n",
    "    return job_descriptions, all_URLS, bad_URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 1: Create the corpus for two very different job descriptions:  'data scientist' and 'restaurant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+scientist\n",
      "type(num_jobs_area)\n",
      "There were 1790 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Done with collecting the job postings!\n",
      "There were 27 jobs successfully found.\n",
      "Searching for  waiter\n",
      "type(num_jobs_area)\n",
      "There were 1476 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Done with collecting the job postings!\n",
      "There were 44 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus, URLs, bad_URLs = create_corpus(city = 'london',job_list=['data+scientist', 'waiter'],pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44 job postings, 44 URLS\n"
     ]
    }
   ],
   "source": [
    "print('There are {0} job postings, {1} URLS'.format(len(corpus), len(URLs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_stop_word_cleaner(corpus, stop_words_input=None):\n",
    "    '''\n",
    "    This function just removes some words from the corpus in case you realize you want to filter out more words.\n",
    "    Inputs: corpus\n",
    "    Outputs: stop_words filtered out of corpus\n",
    "    '''\n",
    "    cc=[0]\n",
    "    if type(corpus) != list: \n",
    "        cc[0] = corpus\n",
    "        corpus = cc\n",
    "        \n",
    "    for ic,text_original in enumerate(corpus):       \n",
    "        stop_words_base = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "        if stop_words_input != None: stop_words_base = stop_words_base.union(stop_words_input)\n",
    "        stop_words_jobs = set(['job','jobs','candidate','candidates','apply','now','skills','application','new',\n",
    "                           'group','day','company','experience','our','job','position',\n",
    "                           'pay','train','training','team','staff','indeed','work','working',\n",
    "                           'yes','we','us','pay','hour','hours','uk','london','hire',\n",
    "                           'team','within','slavery','therefore','opportunities','opportunity',\n",
    "                           'motivation','motivated','he','she','he/she','much','very',\n",
    "                              'cookies','com','asos','postcode','ago','date','benefits',\n",
    "                              'cv','role','cookies','com','asos','postcode','ago','date',\n",
    "                               'benefits','religion','sexual','orientation','salary','asap',\n",
    "                               'annum','race','like' ,'may','enjoy','keywords' ])\n",
    "    \n",
    "        stop_words = stop_words_base.union(stop_words_jobs)\n",
    "        text = [w.lower() for w in text_original.split() if w.lower() not in stop_words]\n",
    "        text_original = ' '.join(text)\n",
    "        corpus[ic] = text_original\n",
    "    if len(corpus) == 1 :\n",
    "        return corpus[ic]\n",
    "    else:\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some additional useless words\n",
    "corpus =  corpus_stop_word_cleaner(corpus, ['cv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(corpus, nmin=1,nmax=3,nfeat=10000, mindf=1):    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(nmin,nmax), min_df = mindf, \n",
    "                                 sublinear_tf = True, max_features = nfeat)\n",
    "    job_features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, job_features # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectLondon, london_features = create_features(corpus, nmin=1,nmax=3, nfeat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of extracted features (44, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of extracted features',london_features.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 100)\n"
     ]
    }
   ],
   "source": [
    "print(london_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avp quantitative analyst lma recruitment co skip description searchclose find jobscompany reviewsfin\n"
     ]
    }
   ],
   "source": [
    "print(corpus[3][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use these features to do UNSUPERVISED classification with GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_GMM(k, features):\n",
    "    cov_type='tied'\n",
    "    estimator3 = GaussianMixture(n_components=k,\n",
    "                       covariance_type=cov_type, max_iter=20, random_state=0  )\n",
    "    estimator3.fit(features )    # Learns model parameters\n",
    "    y_pred = estimator3.predict(features)\n",
    "    return y_pred, estimator3.predict_proba(features), estimator3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "ylab1, yprob1,estimator1=call_GMM(k, london_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the labels we know we did a reasonable good job, since we know all the first job postings are data science related, and the last ones are realted to waiters jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's see which features have more weight  in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = vectLondon.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_features(k, feature_names,estimator,perc=99.9):\n",
    "    for ik in range(k):        \n",
    "        impor_feat = {}\n",
    "        muik = estimator.means_[ik]\n",
    "        tth = np.percentile(muik,perc)\n",
    "        for i,iv in enumerate(muik):\n",
    "            if iv > tth:\n",
    "                impor_feat[feature_names[i] ] =  iv \n",
    "        sorted_x = sorted(impor_feat.items(), key=operator.itemgetter(1),reverse=True  )\n",
    "        print('\\n \\n Important features for cluster ', ik)\n",
    "        for ii in sorted_x:\n",
    "             print('{0:<30s}{1}'.format(ii[0],ii[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Important features for cluster  0\n",
      "data                          0.28301804731452984\n",
      "business                      0.15629106126922104\n",
      "scientist                     0.1535616342043353\n",
      "help                          0.14529364299440817\n",
      "research                      0.13051616225029833\n",
      "\n",
      " \n",
      " Important features for cluster  1\n",
      "waiter                        0.34898335237009015\n",
      "service                       0.20181304942332462\n",
      "help                          0.15273072464487983\n",
      "reviews                       0.1223173971749404\n",
      "services                      0.09996176199299997\n"
     ]
    }
   ],
   "source": [
    "importance_features(k,feat_names,estimator1,perc=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is not much incertitude in the labeling, since we can see the probability is distributed only on one Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.46166497e-60],\n",
       "       [  1.00000000e+00,   1.73836827e-60],\n",
       "       [  1.00000000e+00,   7.20258416e-62],\n",
       "       [  6.93099134e-57,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   1.26330172e-72],\n",
       "       [  1.00000000e+00,   2.77186522e-59],\n",
       "       [  1.00000000e+00,   3.70681868e-60],\n",
       "       [  1.00000000e+00,   1.61400038e-59],\n",
       "       [  1.00000000e+00,   1.69941581e-60],\n",
       "       [  1.00000000e+00,   1.03947753e-56]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yprob1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look only at data-science jobs and see what features sets them appart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for  data+science\n",
      "type(num_jobs_area)\n",
      "There were 5326 jobs found, london\n",
      "Getting page 0\n",
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Done with collecting the job postings!\n",
      "There were 75 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "corpus_ds, URLs_ds, bad_URLs_ds = create_corpus(city = 'london',job_list=['data+science'],pages=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_ds =  corpus_stop_word_cleaner(corpus_ds,['ck','best','great','boyce','mendez','mondelez','durlston','cwjobs','futureheads','harnham','venturi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 500)\n"
     ]
    }
   ],
   "source": [
    "vectLondon_ds, london_features_ds = create_features(corpus_ds, nmin=1,nmax=3,nfeat=500, mindf=5)\n",
    "feat_names_ds = vectLondon_ds.get_feature_names()\n",
    "print(london_features_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels [3 1 3 1 2 1 1 1 2 0 2 0 1 1 1 3 3 1 1 1 3 2 1 1 1 2 3 1 0 1 2 1 1 1 3 1 1\n",
      " 2 1 1 2 2 2 1 0 1 1 1 1 1 2 0 1 2 2 3 2 1 3 2 0 1 1 1 2 2 1 1 3 1 2 1 1 1\n",
      " 1]\n",
      "Probability of belonging to each cluster [[  4.91272164e-42   1.47300082e-01   8.24583506e-36   8.52699918e-01]\n",
      " [  1.72533774e-62   9.99947673e-01   5.00420735e-53   5.23273617e-05]\n",
      " [  8.55562609e-43   1.23105622e-01   4.17968854e-37   8.76894378e-01]\n",
      " [  5.92908971e-54   9.97533813e-01   8.29750436e-46   2.46618681e-03]\n",
      " [  4.39147798e-03   4.13869091e-45   9.95608522e-01   3.24111757e-28]]\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "zlab,zprob, estimn = call_GMM(k, london_features_ds.toarray() )\n",
    "print('Labels',zlab)\n",
    "print('Probability of belonging to each cluster', zprob[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Important features for cluster  0\n",
      "research                      0.13519889107029157\n",
      "sciences                      0.09332351293807206\n",
      "social                        0.08184207446558818\n",
      "data                          0.0771361762386771\n",
      "content                       0.07563418282231256\n",
      "\n",
      " \n",
      " Important features for cluster  1\n",
      "analyst                       0.1040533220599665\n",
      "analytics                     0.09383322420589418\n",
      "business                      0.08764275289464411\n",
      "data                          0.08016894058385425\n",
      "customer                      0.07747423994838035\n",
      "\n",
      " \n",
      " Important features for cluster  2\n",
      "business                      0.060880200267596746\n",
      "data                          0.0602169703252302\n",
      "engineer                      0.0595723383638893\n",
      "help                          0.056738072390410184\n",
      "projects                      0.05558856031452268\n",
      "\n",
      " \n",
      " Important features for cluster  3\n",
      "developer                     0.16206101848635407\n",
      "software                      0.12119626940241453\n",
      "java                          0.11103844485126925\n",
      "selection                     0.10707740528104592\n",
      "coding                        0.0893526401058581\n"
     ]
    }
   ],
   "source": [
    "importance_features(k, feat_names_ds, estimn, perc=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If fewer features were to be considered, the probabilites are distributed slightly different. For example, consider only 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "Probability of belonging to each cluster [[  4.91272164e-42   1.47300082e-01   8.24583506e-36   8.52699918e-01]\n",
      " [  1.72533774e-62   9.99947673e-01   5.00420735e-53   5.23273617e-05]\n",
      " [  8.55562609e-43   1.23105622e-01   4.17968854e-37   8.76894378e-01]\n",
      " [  5.92908971e-54   9.97533813e-01   8.29750436e-46   2.46618681e-03]\n",
      " [  4.39147798e-03   4.13869091e-45   9.95608522e-01   3.24111757e-28]\n",
      " [  9.37917404e-59   9.99509611e-01   7.20770495e-49   4.90389271e-04]\n",
      " [  2.67654269e-55   9.99956066e-01   1.57547517e-42   4.39342998e-05]\n",
      " [  2.77135086e-54   9.99619710e-01   1.90500343e-42   3.80290336e-04]\n",
      " [  5.23434948e-04   1.26941061e-59   9.99476565e-01   1.30482674e-41]\n",
      " [  9.82504397e-01   1.58152226e-53   1.74956033e-02   3.11998608e-33]]\n"
     ]
    }
   ],
   "source": [
    "vectLondon_ds, london_features_ds = create_features(corpus_ds, nmin=1,nmax=1,nfeat=4, mindf=1)\n",
    "feat_names_ds = vectLondon_ds.get_feature_names()\n",
    "print(london_features_ds.shape)\n",
    "k=4\n",
    "zlab,zprob, estimn = call_GMM(k, london_features_ds.toarray() )\n",
    "#print('Labels',zlab)\n",
    "print('Probability of belonging to each cluster', zprob[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at each category, and find the best one(s) suited for you. \n",
    "\n",
    "### Now, you're job search is much easier!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def provide_url_one_class(label,URL, target_category):\n",
    "    target_url=[]\n",
    "    for i,ilab in enumerate(label):\n",
    "        if ilab in target_category:\n",
    "            print(URL[i])\n",
    "            target_url.append(URL[i])\n",
    "    return target_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.co.uk/rc/clk?jk=3e332c559a53025f&fccid=d8589ac02ce8d5f2&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=3e332c559a53025f&fccid=d8589ac02ce8d5f2&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=698df7625ba7b9af&fccid=618c59020c53801c&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=02aa83986bb9c4e6&fccid=bcbe3f5328b59f6d&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=e6474828284e2918&fccid=892c145157842ceb&vjs=3\n",
      "https://www.indeed.co.uk/rc/clk?jk=41f65aa7d234422d&fccid=f21bb40e0e79c0f9&vjs=3\n"
     ]
    }
   ],
   "source": [
    "#lets look at developer-oriented data science jobs\n",
    "my_url = provide_url_one_class(zlab,URLs_ds, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at estimator2.weights_   we can see which Gaussians have the largest contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08663011,  0.53281582,  0.23336989,  0.14718418])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimn.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
